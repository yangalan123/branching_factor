@comment{{This file has been generated by bib2bib 1.99}}

@comment{{Command line: bib2bib -c '$key="saparov2023language" or $key="lin2023unlocking" or $key="dubey2024llama" or $key="merrill2023parallelism" or $key="chakrabarty2024art" or $key="gupta2024language" or $key="li2024predicting" or $key="perez2022red" or $key="jiang2021can" or $key="guo2024benchmarking" or $key="brendan2013perplexity" or $key="bai2022training" or $key="kuhn2023semantic" or $key="wu2024language" or $key="tevet2021evaluating" or $key="renze2024effect" or $key="yao2023tree" or $key="zhou2024lima" or $key="wei2022chain" or $key="cover1999elements" or $key="hill1973diversity" or $key="Holtzman2020The" or $key="pal2023future" or $key="verma2023revisiting" or $key="jaeger2006speakers" or $key="liu-etal-2022-rethinking" or $key="song2024good" or $key="tian2024large" or $key="farquhar2024detecting" or $key="li2016diversity" or $key="touvron2023llama" or $key="kirk2024understanding" or $key="ye2024benchmarking" or $key="levy2008expectation" or $key="wang2022uncertainty" or $key="cai2024medusa" or $key="hewitt2022truncation" or $key="wan2024alphazero" or $key="shi2024detecting" or $key="fu2023chain" or $key="west2025base" or $key="yang2024qwen2" or $key="kadavath2022language" or $key="welleck2024from" or $key="jost2006entropy" or $key="ouyang2022training" or $key="genzel2002entropy" or $key="chen2022cognac" or $key="padmakumar2024does" or $key="mahowald2013info" or $key="miller1995wordnet" or $key="lu2025ai" or $key="guo2025deepseek" or $key="kwon2023efficient" or $key="mudireddy2024slaves" or $key="li2024latesteval" or $key="meister2021revisiting" or $key="shannon1948mathematical" or $key="hao2023reasoning" or $key="cohen2015markov" or $key="desai2020calibration" or $key="bigelowsubjective" or $key="hendrycks2021measuring" or $key="fei2024nudging" or $key="xiong2024can" or $key="breiman1957individual" or 1=2' ./example_paper.bib}}

@inproceedings{wei2022chain,
  title = {Chain-of-thought prompting elicits reasoning in large language models},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  booktitle = {NeurIPS},
  year = {2022}
}

@article{hill1973diversity,
  title = {Diversity and evenness: a unifying notation and its consequences},
  author = {Hill, Mark O},
  journal = {Ecology},
  volume = {54},
  number = {2},
  pages = {427--432},
  year = {1973},
  publisher = {Wiley Online Library}
}

@article{jost2006entropy,
  title = {Entropy and diversity},
  author = {Jost, Lou},
  journal = {Oikos},
  volume = {113},
  number = {2},
  pages = {363--375},
  year = {2006},
  publisher = {Wiley Online Library}
}

@book{cover1999elements,
  title = {Elements of information theory},
  author = {Cover, Thomas M},
  year = {1999},
  publisher = {John Wiley \& Sons}
}

@inproceedings{Holtzman2020The,
  title = {The Curious Case of Neural Text Degeneration},
  author = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
  booktitle = {International Conference on Learning Representations},
  year = {2020},
  url = {https://openreview.net/forum?id=rygGQyrFvH}
}

@inproceedings{hewitt2022truncation,
  title = {Truncation Sampling as Language Model Desmoothing},
  author = {Hewitt, John and Manning, Christopher D and Liang, Percy},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages = {3414--3427},
  year = {2022}
}

@inproceedings{chen2022cognac,
  title = {{Cognac}: Controllable Text Generation with Language Constraints},
  author = {Chen, Howard and Li, Huihan and Chen, Danqi and Narasimhan, Karthik},
  booktitle = {arXiv},
  year = {2022}
}

@article{miller1995wordnet,
  title = {WordNet: a lexical database for English},
  author = {Miller, George A},
  journal = {Communications of the ACM},
  volume = {38},
  number = {11},
  pages = {39--41},
  year = {1995},
  publisher = {ACM New York, NY, USA}
}

@inproceedings{chakrabarty2024art,
  title = {Art or artifice? large language models and the false promise of creativity},
  author = {Chakrabarty, Tuhin and Laban, Philippe and Agarwal, Divyansh and Muresan, Smaranda and Wu, Chien-Sheng},
  booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages = {1--34},
  year = {2024}
}

@article{farquhar2024detecting,
  title = {Detecting hallucinations in large language models using semantic entropy},
  author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  journal = {Nature},
  volume = {630},
  number = {8017},
  pages = {625--630},
  year = {2024},
  publisher = {Nature Publishing Group UK London}
}

@article{touvron2023llama,
  title = {Llama 2: Open foundation and fine-tuned chat models},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year = {2023}
}

@article{dubey2024llama,
  title = {The llama 3 herd of models},
  author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal = {arXiv preprint arXiv:2407.21783},
  year = {2024}
}

@article{bai2022training,
  title = {Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal = {arXiv preprint arXiv:2204.05862},
  year = {2022}
}

@article{ouyang2022training,
  title = {Training language models to follow instructions with human feedback},
  author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal = {Advances in neural information processing systems},
  volume = {35},
  pages = {27730--27744},
  year = {2022}
}

@misc{brendan2013perplexity,
  author = {Brendan O'Connor},
  title = {Perplexity as Branching Factor as Shannon Diversity Index},
  year = {2013},
  url = {https://brenocon.com/blog/2013/01/perplexity-as-branching-factor-as-shannon-diversity-index/},
  note = {Accessed: 2024-09-28}
}

@inproceedings{li2024latesteval,
  title = {Latesteval: Addressing data contamination in language model evaluation through dynamic and time-sensitive test construction},
  author = {Li, Yucheng and Guerin, Frank and Lin, Chenghua},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  pages = {18600--18607},
  year = {2024}
}

@inproceedings{shi2024detecting,
  title = {Detecting Pretraining Data from Large Language Models},
  author = {Shi, Weijia and Ajith, Anirudh and Xia, Mengzhou and Huang, Yangsibo and Liu, Daogao and Blevins, Terra and Chen, Danqi and Zettlemoyer, Luke},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2024}
}

@inproceedings{hendrycks2021measuring,
  title = {Measuring Massive Multitask Language Understanding},
  author = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  booktitle = {International Conference on Learning Representations},
  year = {2021},
  url = {https://openreview.net/forum?id=d7KBjmI3GmQ}
}

@article{fu2023chain,
  title = {Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance},
  author = {Fu, Yao and Ou, Litu and Chen, Mingyu and Wan, Yuhao and Peng, Hao and Khot, Tushar},
  journal = {arXiv preprint arXiv:2305.17306},
  year = {2023}
}

@inproceedings{tian2024large,
  title = {Are Large Language Models Capable of Generating Human-Level Narratives?},
  author = {Tian, Yufei and Huang, Tenghao and Liu, Miri and Jiang, Derek and Spangher, Alexander and Chen, Muhao and May, Jonathan and Peng, Nanyun},
  booktitle = {EMNLP},
  year = {2024}
}

@inproceedings{bigelowsubjective,
  title = {In-Context Learning Dynamics with Random Binary Sequences},
  author = {Eric J Bigelow and Ekdeep Singh Lubana and Robert P. Dick and Hidenori Tanaka and Tomer Ullman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2024},
  url = {https://openreview.net/forum?id=62K7mALO2q}
}

@article{ye2024benchmarking,
  title = {Benchmarking llms via uncertainty quantification},
  author = {Ye, Fanghua and Yang, Mingming and Pang, Jianhui and Wang, Longyue and Wong, Derek F and Yilmaz, Emine and Shi, Shuming and Tu, Zhaopeng},
  journal = {arXiv preprint arXiv:2401.12794},
  year = {2024}
}

@inproceedings{xiong2024can,
  title = {Can {LLM}s Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in {LLM}s},
  author = {Miao Xiong and Zhiyuan Hu and Xinyang Lu and YIFEI LI and Jie Fu and Junxian He and Bryan Hooi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2024},
  url = {https://openreview.net/forum?id=gjeQKFxFpZ}
}

@inproceedings{kuhn2023semantic,
  title = {Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation},
  author = {Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  year = {2023},
  booktitle = {The Eleventh International Conference on Learning Representations}
}

@article{kadavath2022language,
  title = {Language models (mostly) know what they know},
  author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal = {arXiv preprint arXiv:2207.05221},
  year = {2022}
}

@article{desai2020calibration,
  title = {Calibration of pre-trained transformers},
  author = {Desai, Shrey and Durrett, Greg},
  journal = {arXiv preprint arXiv:2003.07892},
  year = {2020}
}

@article{jiang2021can,
  title = {How can we know when language models know? on the calibration of language models for question answering},
  author = {Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {962--977},
  year = {2021},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{wang2022uncertainty,
  title = {Uncertainty estimation and reduction of pre-trained models for text regression},
  author = {Wang, Yuxia and Beck, Daniel and Baldwin, Timothy and Verspoor, Karin},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {680--696},
  year = {2022},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{gupta2024language,
  title = {Language Model Cascades: Token-Level Uncertainty And Beyond},
  author = {Gupta, Neha and Narasimhan, Harikrishna and Jitkrittum, Wittawat and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv},
  year = {2024},
  booktitle = {The Twelfth International Conference on Learning Representations}
}

@inproceedings{wu2024language,
  title = {Do language models plan ahead for future tokens?},
  author = {Wu, Wilson and Morris, John X and Levine, Lionel},
  booktitle = {The First Conference on Language Modeling},
  year = {2024}
}

@inproceedings{pal2023future,
  title = {Future Lens: Anticipating Subsequent Tokens from a Single Hidden State},
  author = {Pal, Koyena and Sun, Jiuding and Yuan, Andrew and Wallace, Byron C and Bau, David},
  booktitle = {Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)},
  pages = {548--560},
  year = {2023}
}

@article{fei2024nudging,
  title = {Nudging: Inference-time Alignment via Model Collaboration},
  author = {Fei, Yu and Razeghi, Yasaman and Singh, Sameer},
  journal = {arXiv preprint arXiv:2410.09300},
  year = {2024}
}

@inproceedings{lin2023unlocking,
  title = {The unlocking spell on base llms: Rethinking alignment via in-context learning},
  author = {Lin, Bill Yuchen and Ravichander, Abhilasha and Lu, Ximing and Dziri, Nouha and Sclar, Melanie and Chandu, Khyathi and Bhagavatula, Chandra and Choi, Yejin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2023}
}

@article{zhou2024lima,
  title = {Lima: Less is more for alignment},
  author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  year = {2024}
}

@article{levy2008expectation,
  title = {Expectation-based syntactic comprehension},
  author = {Levy, Roger},
  journal = {Cognition},
  volume = {106},
  number = {3},
  pages = {1126--1177},
  year = {2008},
  publisher = {Elsevier}
}

@article{jaeger2006speakers,
  title = {Speakers optimize information density through syntactic reduction},
  author = {Jaeger, T and Levy, Roger},
  journal = {Advances in neural information processing systems},
  volume = {19},
  year = {2006}
}

@article{renze2024effect,
  title = {The effect of sampling temperature on problem solving in large language models},
  author = {Renze, Matthew and Guven, Erhan},
  journal = {arXiv preprint arXiv:2402.05201},
  year = {2024}
}

@article{song2024good,
  title = {The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism},
  author = {Song, Yifan and Wang, Guoyin and Li, Sujian and Lin, Bill Yuchen},
  journal = {arXiv preprint arXiv:2407.10457},
  year = {2024}
}

@inproceedings{saparov2023language,
  title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
  author = {Abulhair Saparov and He He},
  booktitle = {The Eleventh International Conference on Learning Representations },
  year = {2023},
  url = {https://openreview.net/forum?id=qFVVBzXxR2V}
}

@inproceedings{padmakumar2024does,
  title = {Does Writing with Language Models Reduce Content Diversity?},
  author = {Vishakh Padmakumar and He He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2024},
  url = {https://openreview.net/forum?id=Feiz5HtCD0}
}

@inproceedings{cai2024medusa,
  title = {Medusa: Simple {LLM} Inference Acceleration Framework with Multiple Decoding Heads},
  author = {Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
  booktitle = {Forty-first International Conference on Machine Learning},
  year = {2024},
  url = {https://openreview.net/forum?id=PEpbUobfJv}
}

@article{welleck2024from,
  title = {From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models},
  author = {Sean Welleck and Amanda Bertsch and Matthew Finlayson and Hailey Schoelkopf and Alex Xie and Graham Neubig and Ilia Kulikov and Zaid Harchaoui},
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  year = {2024},
  url = {https://openreview.net/forum?id=eskQMcIbMS},
  note = {Survey Certification}
}

@inproceedings{tevet2021evaluating,
  title = {Evaluating the Evaluation of Diversity in Natural Language Generation},
  author = {Tevet, Guy and Berant, Jonathan},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages = {326--346},
  year = {2021}
}

@inproceedings{li2016diversity,
  title = {A Diversity-Promoting Objective Function for Neural Conversation Models},
  author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, William B},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {110--119},
  year = {2016}
}

@article{guo2024benchmarking,
  title = {Benchmarking Linguistic Diversity of Large Language Models},
  author = {Guo, Yanzhu and Shang, Guokan and Clavel, Chlo{\'e}},
  journal = {arXiv preprint arXiv:2412.10271},
  year = {2024}
}

@inproceedings{kirk2024understanding,
  title = {Understanding the Effects of {RLHF} on {LLM} Generalisation and Diversity},
  author = {Robert Kirk and Ishita Mediratta and Christoforos Nalmpantis and Jelena Luketina and Eric Hambro and Edward Grefenstette and Roberta Raileanu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2024},
  url = {https://openreview.net/forum?id=PXD3FAVHJT}
}

@inproceedings{liu-etal-2022-rethinking,
  title = {Rethinking and Refining the Distinct Metric},
  author = {Liu, Siyang  and
      Sabour, Sahand  and
      Zheng, Yinhe  and
      Ke, Pei  and
      Zhu, Xiaoyan  and
      Huang, Minlie},
  editor = {Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month = may,
  year = {2022},
  address = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2022.acl-short.86/},
  doi = {10.18653/v1/2022.acl-short.86},
  pages = {762--770}
}

@article{guo2025deepseek,
  title = {Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author = {Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal = {arXiv preprint arXiv:2501.12948},
  year = {2025}
}

@article{mudireddy2024slaves,
  title = {Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models},
  author = {Mudireddy, Avinash and Bell, Tyler and Mudumbai, Raghu},
  journal = {arXiv preprint arXiv:2405.13798},
  year = {2024}
}

@article{shannon1948mathematical,
  title = {A mathematical theory of communication},
  author = {Shannon, Claude E},
  journal = {The Bell system technical journal},
  volume = {27},
  number = {3},
  pages = {379--423},
  year = {1948},
  publisher = {Nokia Bell Labs}
}

@article{breiman1957individual,
  title = {The individual ergodic theorem of information theory},
  author = {Breiman, Leo},
  journal = {The Annals of Mathematical Statistics},
  volume = {28},
  number = {3},
  pages = {809--811},
  year = {1957},
  publisher = {JSTOR}
}

@article{mahowald2013info,
  title = {Info/information theory: Speakers choose shorter words in predictive contexts},
  author = {Mahowald, Kyle and Fedorenko, Evelina and Piantadosi, Steven T and Gibson, Edward},
  journal = {Cognition},
  volume = {126},
  number = {2},
  pages = {313--318},
  year = {2013},
  publisher = {Elsevier}
}

@article{meister2021revisiting,
  title = {Revisiting the uniform information density hypothesis},
  author = {Meister, Clara and Pimentel, Tiago and Haller, Patrick and J{"a}ger, Lena and Cotterell, Ryan and Levy, Roger},
  journal = {arXiv preprint arXiv:2109.11635},
  year = {2021}
}

@inproceedings{verma2023revisiting,
  title = {Revisiting Entropy Rate Constancy in Text},
  author = {Verma, Vivek and Tomlin, Nicholas and Klein, Dan},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages = {15537--15549},
  year = {2023}
}

@inproceedings{genzel2002entropy,
  title = {Entropy rate constancy in text},
  author = {Genzel, Dmitriy and Charniak, Eugene},
  booktitle = {Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages = {199--206},
  year = {2002}
}

@article{cohen2015markov,
  title = {Markov's inequality and Chebyshev's inequality for tail probabilities: A sharper image},
  author = {Cohen, Joel E},
  journal = {The American Statistician},
  volume = {69},
  number = {1},
  pages = {5--7},
  year = {2015},
  publisher = {Taylor \& Francis}
}

@article{merrill2023parallelism,
  title = {The parallelism tradeoff: Limitations of log-precision transformers},
  author = {Merrill, William and Sabharwal, Ashish},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {531--545},
  year = {2023},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{yang2024qwen2,
  title = {Qwen2 technical report},
  author = {Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal = {arXiv preprint arXiv:2407.10671},
  year = {2024}
}

@article{li2024predicting,
  title = {Predicting vs. acting: A trade-off between world modeling \& agent modeling},
  author = {Li, Margaret and Shi, Weijia and Pagnoni, Artidoro and West, Peter and Holtzman, Ari},
  journal = {arXiv preprint arXiv:2407.02446},
  year = {2024}
}

@inproceedings{lu2025ai,
  title = {{AI} as Humanity{\textquoteright}s Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text},
  author = {Ximing Lu and Melanie Sclar and Skyler Hallinan and Niloofar Mireshghallah and Jiacheng Liu and Seungju Han and Allyson Ettinger and Liwei Jiang and Khyathi Chandu and Nouha Dziri and Yejin Choi},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  year = {2025},
  url = {https://openreview.net/forum?id=ilOEOIqolQ}
}

@inproceedings{hao2023reasoning,
  title = {Reasoning with Language Model is Planning with World Model},
  author = {Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua and Wang, Zhen and Wang, Daisy and Hu, Zhiting},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages = {8154--8173},
  year = {2023}
}

@inproceedings{wan2024alphazero,
  title = {Alphazero-like tree-search can guide large language model decoding and training},
  author = {Wan, Ziyu and Feng, Xidong and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun},
  booktitle = {Forty-first International Conference on Machine Learning},
  year = {2024}
}

@article{yao2023tree,
  title = {Tree of thoughts: Deliberate problem solving with large language models},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal = {Advances in neural information processing systems},
  volume = {36},
  pages = {11809--11822},
  year = {2023}
}

@inproceedings{perez2022red,
  title = {Red Teaming Language Models with Language Models},
  author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages = {3419--3448},
  year = {2022}
}

@article{west2025base,
  title = {Base Models Beat Aligned Models at Randomness and Creativity},
  author = {West, Peter and Potts, Christopher},
  journal = {arXiv preprint arXiv:2505.00047},
  year = {2025}
}

@inproceedings{kwon2023efficient,
  title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle = {Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year = {2023}
}

