<!doctype html>
<html lang="en">
<head>
    <title>How Alignment Shrinks the Generative Horizon</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:title" content="How Alignment Shrinks the Generative Horizon" />
    <meta property="og:description" content="We investigate how alignment tuning affects the diversity of LLM outputs through the lens of probability concentration, introducing the Branching Factor as a diagnostic measure." />
    <meta property="og:url" content="https://your-website-url.com/" />
    <meta property="og:image" content="static/visualization/surface_figure_1_v2.pdf" />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:description" content="We investigate how alignment tuning affects the diversity of LLM outputs through the lens of probability concentration, introducing the Branching Factor as a diagnostic measure." />
    <meta name="twitter:url" content="https://your-website-url.com/" />
    <meta name="twitter:image" content="static/visualization/surface_figure_1_v2.pdf" />
    
    <script src="js/template.v2.js"></script>
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://d3js.org/d3-collection.v1.min.js"></script>
    <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
    <script src="js/cross_fade.js"></script>
    <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.35.2/gradio.js"></script>
    <link rel="stylesheet" href="css/style.css">
    <link rel="icon" type="favicon/png" href="favicon.png">

    <!-- MathJax v2 for high-quality math rendering -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ['\\(','\\)'] ],
                displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
                processEscapes: true,
                processEnvironments: true
            },
            "HTML-CSS": {
                availableFonts: ["STIX"],
                linebreaks: { automatic: true },
                imageFont: null
            },
            SVG: {
                font: "STIX-Web",
                linebreaks: { automatic: true },
                imageFont: null
            },
            showProcessingMessages: false,
            messageStyle: "none"
        });
    </script>
    
    <!-- PDF.js for PDF rendering -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.4.120/pdf.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.21/lodash.min.js"></script>
    <script src="js/main.js"></script>
</head>

<body>
<div class="header-container" id="top">
    <div class="header-content">
        <h1>How Alignment Shrinks the Generative Horizon</h1>
        <h2>Understanding LLM Output Diversity Through Probability Concentration</h2>
        <div class="button-container">
            <a href="https://arxiv.org/abs/2502.16810" class="button">Paper</a>
            <a href="#introduction" class="button">Codes</a>
            <a href="#results" class="button">Video</a>
            <a href="#results" class="button">Results</a>
        </div>
    </div>
    <div class="header-image">
        <div class="img-container lazy-load" data-src="static/visualization/surface_figure_1_v2.pdf">
            <div class="loading-spinner">Loading...</div>
        </div>
    </div>
</div>

<d-article>
    <d-contents>
        <nav>
            <h4>Contents</h4>
            <div><a href="#top">Top</a></div>
            <div><a href="#abstract">LLM Branching Factor (BF)</a></div>
            <!-- <div><a href="#introduction">Introduction</a></div> -->
            <!-- <div><a href="#preliminary">Background</a></div> -->
            <div><a href="#sampling-efforts">Case Study: Is Decoding Methods Still Crucial for Aligned Models?</a></div>
            <div><a href="#bf-formulation">How to Compute BF?</a></div>
            <div><a href="#bf-measuring">What impacts BF?</a></div>
            <div><a href="#application-variance">Application: Variance Reduction for Reasoning</a></div>
            <div><a href="#bf-nudging">How does Alignment Tuning Impact BF?</a></div>
            <!-- <div><a href="#related-works">Related Works</a></div>
            <div><a href="#conclusion">Conclusion</a></div> -->
        </nav>
    </d-contents>

    <div class="main-content">
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p><a href="https://yangalan123.github.io/" class="author-link">Chenghao Yang</a> and <a href="https://ariholtzman.com/" class="author-link">Ari Holtzman</a></p>
                </div>
            </div>
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p>University of Chicago</p>
                </div>
            </div>
        </div>

        <section id="abstract">
            <h2>Investigating LLM Probability Concentration via Branching Factor (BF)</h2>
            <p>
                Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution.
            </p>
            <p>
                To quantify this concentration, we introduce the <em>Branching Factor</em> (BF)--a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings:
            </p>
            <ul>
                <li>BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate</li>
                <li>Alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models</li>
            </ul>
            <div class="figure-container">
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/surface_figure_1_v2.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                </figure>
            </div>
            <p>
                We illustrate the concept in the figure above. Building on this insight, we find this stability has surprising implications for complex reasoning.
            </p>
            <blockquote class="highlight-quote">
                <em>Aligned Chain-of-Thought (CoT) models</em> (e.g., DeepSeek-distilled models) leverage this effect by generating longer reasoning chains, pushing generation into later, more deterministic (lower BF) stages, resulting in more stable outputs.
            </blockquote>
            <p>
                We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF.
            </p>
            <p>
                Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - shedding light on 
            </p>
            <ul>
                <li>How Alignment reduces variability</li>
                <li>How CoT promotes stable generations</li>
                <li>How base models can be steered away from diversity</li>
            </ul>
        </section>

        <section id="preliminary" class="collapsible-section" data-collapsed="true">
            <div class="collapsible-header">
                <h3>Preliminary</h3>
                <button class="collapsible-toggle collapsed" aria-label="Toggle Background section"></button>
            </div>
            <div class="collapsible-content collapsed">
                <h3>Autoregressive Language Models</h3>
                <p>
                    LLMs are typically trained to predict the next token and the probability of output $P\left(y_{1:N} | x; \theta \right)$ can be decomposed as: $P\left(y_{1:N} | x; \theta \right)=\prod_{t=1}^{N}P\left(y_t | [x, y_{1:t-1}]; \theta \right)$,
                    where $y_{1:t-1}$ is the output up to position $t-1$, $\theta$ is the model parameter, and $x$ is the prompt.
                    Each output sample is generated via token-by-token sampling, and the generation of multiple samples naturally forms a search tree <d-cite key="yao2023tree"></d-cite><d-cite key="hao2023reasoning"></d-cite><d-cite key="wan2024alphazero"></d-cite>.
                    Modern LLMs would go through multiple training stages. 
                    In this paper, we would use <em>base models</em> to refer to the models trained without <em>alignment tuning</em> techniques <d-cite key="touvron2023llama"></d-cite>, including instruction tuning and Reinforcement Learning from Human Feedback (RLHF) <d-cite key="ouyang2022training"></d-cite><d-cite key="bai2022training"></d-cite> (e.g., "Llama-2-13B" <d-cite key="touvron2023llama"></d-cite>) and <em>aligned models</em> to refer to models undergoing these additional fine-tuning stages (e.g., "Llama-2-13B-Chat").
                </p>
                
                <h3>Decoding Methods as Truncated Sampling</h3>
                <p>
                    Though LLMs are trained with a large vocabulary size $|V|$, in many cases, the desired tokens often concentrate on a much smaller set of tokens under distribution $P(y_{t} | x, y_{1:t-1}; \theta)$. Common decoding methods <d-cite key="Holtzman2020The"></d-cite><d-cite key="hewitt2022truncation"></d-cite>
                    utilize this observation and propose various heuristics to truncate vocabulary $V$ as $V_t$ at each step $t$. The next token is then sampled from the renormalized distribution $\tilde{P}\left(y_t | [x, y_{1:t-1}]; \theta \right)$:
                </p>
                <div class="equation-container">
                    <h4>$$\tilde{P}\left(y_t | [x, y_{1:t-1}]; \theta \right) = \begin{cases}
                         \frac{P(y_{t} | x, y_{1:t-1}; \theta)}{\sum_{y_{t} \in V_t} P(y_{t} | x, y_{1:t-1}; \theta) } & y_{t} \in V_t \\
                         0 & \text{otherwise}
                    \end{cases}$$</h4>
                </div>
                
                <h3>Token-wise Conditional Entropy</h3>
                <p>
                    Since tokens are sampled from the truncated distribution $\tilde{P}$, we use $\tilde{P}$ to compute the token-level conditional entropy $\tilde{H}$ for a given prefix instance $y_{1:t-1}$:
                </p>
                <div class="equation-container">
                    <h4>$$\tilde{H}\left(Y_t | [x, y_{1:t-1}]; \theta \right) 
                    =-\sum_{y_t} \tilde{P}\left(y_t | [x, y_{1:t-1}]; \theta \right) \log \tilde{P}\left(y_t | [x, y_{1:t-1}]; \theta \right)$$</h4>
                </div>
                <p>
                    To generalize, we can compute the <em>expected conditional entropy</em> over the distribution of prefix sequences $Y_{1:t-1}$: $\tilde{H}\left(Y_t | [x, Y_{1:t-1}]; \theta \right)=\mathbb{E}_{y_{1:t-1}}\tilde{H}\left(Y_t | [x, y_{1:t-1}]; \theta \right)$.
                </p>
            </div>
        </section>

        <section id="sampling-efforts">
            <h2>Case Study: Is Decoding Method Still Crucial for Modern LLMs?</h2>
            <p>
                Many prevalent decoding methods were introduced before LLMs scaled to billions of parameters and underwent multiple training stages. Additionally, model developers adopt different decoding strategies when reporting LLM capabilities <d-cite key="touvron2023llama"></d-cite><d-cite key="dubey2024llama"></d-cite><d-cite key="yang2024qwen2"></d-cite><d-cite key="guo2025deepseek"></d-cite>, raising questions about the significance of decoding choices for modern LLMs. To explore this, we benchmark various decoding methods on standard LLM reasoning tasks, extending prior work <d-cite key="song2024good"></d-cite><d-cite key="renze2024effect"></d-cite> to the latest models including DeepSeek-distilled models <d-cite key="guo2025deepseek"></d-cite>, which would generate long CoT before the final answer. Specifically, we evaluate model performance on MMLU-STEM <d-cite key="hendrycks2021measuring"></d-cite> under CoT prompting across different temperatures ($T=0.6 / 1.0$) in temperature sampling and truncation thresholds ($p=0.9 / 1.0$) in nucleus sampling <d-cite key="Holtzman2020The"></d-cite>.
            </p>

            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Models</th>
                            <th>Default ($T=0.6, p=0.9$)</th>
                            <th>$T=0.6, p=1.0$</th>
                            <th>$T=1.0, p=0.9$</th>
                            <th>Min ($T=1.0, p=1.0$)</th>
                            <th>$\frac{\text{Default}-\text{Min}}{\text{Default}}\%$</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Llama-3-70B-Instruct</td>
                            <td>78.50 ($\pm$ 2.09)</td>
                            <td>77.60 ($\pm$ 2.23)</td>
                            <td>77.50 ($\pm$ 2.60)</td>
                            <td>75.90 ($\pm$ 2.85)</td>
                            <td>3.31</td>
                        </tr>
                        <tr>
                            <td>Llama-3-70B</td>
                            <td>78.00 ($\pm$ 3.52)</td>
                            <td>74.00 ($\pm$ 3.80)</td>
                            <td>72.00 ($\pm$ 4.38)</td>
                            <td>63.50 ($\pm$ 5.02)</td>
                            <td>18.59</td>
                        </tr>
                        <tr>
                            <td>DeepSeek-R1-Distill-Llama-8B</td>
                            <td>66.30 ($\pm$ 3.51)</td>
                            <td>65.70 ($\pm$ 3.84)</td>
                            <td>62.70 ($\pm$ 4.14)</td>
                            <td>59.70 ($\pm$ 4.65)</td>
                            <td>9.95</td>
                        </tr>
                        <tr>
                            <td>Llama-3.1-8B-Instruct</td>
                            <td>63.00 ($\pm$ 4.01)</td>
                            <td>61.50 ($\pm$ 4.37)</td>
                            <td>57.50 ($\pm$ 4.92)</td>
                            <td>50.50 ($\pm$ 5.34)</td>
                            <td>19.84</td>
                        </tr>
                        <tr>
                            <td>Llama-3.1-8B</td>
                            <td>54.00 ($\pm$ 4.61)</td>
                            <td>53.50 ($\pm$ 4.92)</td>
                            <td>47.00 ($\pm$ 5.21)</td>
                            <td>37.00 ($\pm$ 5.48)</td>
                            <td><strong>31.48</strong></td>
                        </tr>
                    </tbody>
                </table>
                <p class="table-caption"><strong>Table 1:</strong> Experiment Results across decoding methods on STEM subset of MMLU. We follow the common practice of using 5-shot CoT prompting. $\frac{\text{Default}-\text{Min}}{\text{Default}}\%$ indicates the maximum relative performance drop when deviating from the default decoding configuration.</p>
            </div>

            <p>
                The results in Table 1 reveal that for aligned models, decoding configurations have a limited impact -- typically around 10% (up to 20%) relative performance changes. Among Llama-3.1-8B models, DeepSeek-distilled Llama-8B (based on Llama-3.1-8B), which is trained to generate long CoT, exhibits the smallest relative performance changes. In contrast, base models exhibit greater sensitivity, with performance varying by up to 31%. Additionally, lowering the temperature ($T$) generally improves performance across all models more than adjusting truncation threshold ($p$), though excessive reduction (e.g., greedy decoding when $T=0$) may lead to repetition issues <d-cite key="guo2025deepseek"></d-cite>.
            </p>

            <p>
                Based on these observations and findings in existing literature, we propose the following hypotheses:
            </p>

            <div class="hypothesis">
                <h4><strong>Hypothesis 1:</strong></h4>
                <p>Aligned models produce tokens with a more concentrated distribution than base models <d-cite key="padmakumar2024does"></d-cite><d-cite key="bigelowsubjective"></d-cite><d-cite key="lu2025ai"></d-cite><d-cite key="west2025base"></d-cite>.</p>
            </div>

            <div class="hypothesis">
                <h4><strong>Hypothesis 2:</strong></h4>
                <p>Larger models have more concentrated distributions compared with smaller models <d-cite key="ye2024benchmarking"></d-cite><d-cite key="xiong2024can"></d-cite>, though may varied by tasks <d-cite key="lu2025ai"></d-cite><d-cite key="west2025base"></d-cite>.</p>
            </div>

            <div class="hypothesis">
                <h4><strong>Hypothesis 3:</strong></h4>
                <p>As LLMs generate more tokens, its next-word prediction probability distribution becomes increasingly concentrated <d-cite key="tian2024large"></d-cite><d-cite key="chakrabarty2024art"></d-cite>.</p>
            </div>

            <p>
                Researchers often assess probability concentration using token-level metrics such as entropy or log-likelihood. However, these offer only a narrow lens on model behavior: they capture local properties but miss the global structure of the output space--how probability mass is distributed across plausible sequences. This motivates our proposal of the BF as a structural measure of generative breadth.
            </p>
        </section>

        <section id="bf-formulation">
            <h2>Measuring Branching Factor</h2>
            <p>
                The generative process of language models can be viewed as moving down a branching tree, with each token choice selecting a path forward. While the full tree spans $O(|V|^N)$ sequences for vocabulary size $|V|$ and sequence length $N$, LLMs concentrate probability mass on a far smaller subset. To capture how many options the model seriously considers at each step, we introduce the <em>Branching Factor</em> (BF). Given $|\mathcal{T}|$ high-probability leaf sequences, we approximate the tree as a balanced $B$-ary tree, where $B = |\mathcal{T}|^{1/N}$. In this section, we describe how to compute $|\mathcal{T}|$ and $B$ in practice.
            </p>

            <h3>Intuitive Perspective: Exponentiated Entropy (Perplexity) as Branches</h3>
            <p>
                We propose to use the exponentiated entropy (perplexity) to quantify $|\mathcal{T}|$: $|\mathcal{T}| \stackrel{\text{def}}{=} \exp\left({H}(Y_{1:N} | x; \theta)\right)$. This reflects the effective number of equally probable outcomes with the same total uncertainty <d-cite key="brendan2013perplexity"></d-cite>. Analogously, it is like sampling from a fair $|\mathcal{T}|$-sided die, where entropy equals $-\sum \frac{1}{|\mathcal{T}|}\log {\frac{1}{|\mathcal{T}|}} = {H}(Y_{1:N} | x; \theta)$. Thus, $B(x;\theta) = \exp\left({\bar{H}(Y_{1:N} | x; \theta)}\right)$ where ${\bar{H}(Y_{1:N} | x; \theta)}=\frac{1}{N} \tilde{H}\left(Y_{1:N} | x; \theta \right)$ is the averaged entropy per output token up to position $N$. A larger $B(x; \theta)$ indicates a greater potential for diverse outputs.
            </p>
            <p>
                For short outputs, where it's tractable to sample sufficiently many sequences to closely estimate the conditional entropy at each position, we can estimate the BF by computing the conditional entropy at each position and then aggregating as:
            </p>
            <div class="equation-container">
                <h4>$$B(x; \theta) \approx \exp\left(\frac{1}{M} \sum_{i=1}^M \frac{\sum_{t=1}^{|y^{(i)}|} \tilde{H}(Y_t | [x, y_{1:t-1}^{(i)}]; \theta)}{|y^{(i)}|}\right)$$</h4>
            </div>
            <p>
                where $\tilde{H}(Y_t | [x, y_{1:t-1}^{(i)}]; \theta)$ is the entropy of the distribution at position $t$ for sample $i$.
            </p>

            <h3>Practical BF Estimator via Asymptotic Equipartition Property</h3>
            <p>
                While the above approach works well for short outputs, it becomes challenging for longer sequences, as we can only sample a tiny fraction of the exponentially large output space. In such cases, we show that when LLMs generate sufficiently long outputs, the average log-probability of each output sequence will be roughly the same, and can approximate average output entropy well, following the Asymptotic Equipartition Property (AEP) <d-cite key="shannon1948mathematical"></d-cite><d-cite key="breiman1957individual"></d-cite><d-cite key="cover1999elements"></d-cite>. The original AEP proof requires additional assumptions about the generation process, such as that it needs to be stationary and ergodic, often violated by LLMs. But as noted by <d-cite key="mudireddy2024slaves"></d-cite>, these assumptions are unnecessary if we do not require ${\bar{H}\left(Y_{1:N} | x; \theta \right)}$ to converge to a constant:
            </p>

            <div class="theorem">
                <h4><strong>Theorem (AEP for LLMs):</strong></h4>
                <p>Given $0 < \epsilon < 1$, we have:</p>
                <div class="equation-container">
                    <h4>$$\lim_{N \rightarrow \infty}{P\left( \left\lvert -\frac{1}{N}\log \tilde{P}\left(y_{1:N} | x; \theta \right) - {\bar{H}\left(Y_{1:N} | x; \theta \right)}\right\rvert < \epsilon \right) } = 1$$</h4>
                </div>
            </div>

            <p>
                This theorem is equivalent to the statement: for sufficiently large $N$, the probability of any length-$N$ high-probability output $y_{1:N}$ under $\tilde{P}$ can be approximated as $\exp\left(-N\bar{H}(Y_{1:N} | x; \theta)\right)$, rendering log-probability asymptotically ineffective for distinguishing among them.
            </p>

            <!-- Group 2a and 2b together -->
            <div class="figure-group" data-group-name="AEP Verification (NLL vs Entropy)">
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/closeness_loglik_entropy/mmlu/response_mmlu_256_256_0_0.9_Meta-Llama-3-8B-Instruct_split_-1_constraint_5.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 2a: MMLU - Length-averaged NLL closely tracks length-averaged Entropy.</figcaption>
                    </figure>
                </d-figure>
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/closeness_loglik_entropy/language_modeling/response_news_1024_max_tokens_2048_2048_0_0.9_Meta-Llama-3-8B-Instruct_split_-1_constraint_3.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 2b: BBC News - Length-averaged NLL closely tracks length-averaged Entropy.</figcaption>
                    </figure>
                </d-figure>
            </div>
            <!-- Group 2c and 2d together -->
            <div class="figure-group" data-group-name="AEP Verification (NLL Standard Deviation)">
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/closeness_loglik_entropy/mmlu/response_mmlu_256_256_0_0.9_Meta-Llama-3-8B-Instruct_split_-1_loglik_std.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 2c: MMLU - Standard deviation of length-averaged NLL diminishes with output length.</figcaption>
                    </figure>
                </d-figure>
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/closeness_loglik_entropy/language_modeling/response_news_1024_max_tokens_2048_2048_0_0.9_Meta-Llama-3-8B-Instruct_split_-1_loglik_std.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 2d: BBC News - Standard deviation of length-averaged NLL diminishes with output length.</figcaption>
                    </figure>
                </d-figure>
            </div>

            <p>
                As an empirical demonstration, we plot the standard deviation of the average negative log-likelihood of Llama-3-8B-Instruct over multiple datasets in Figure 2, where we can see that with the increased output length, the difference between length-averaged entropy and negative log-likelihood (NLL) is reduced, and the standard deviation of average NLL also quickly reduces within the first 50 output tokens.
            </p>

            <p>
                Therefore, for long sequences, we can estimate BF using the NLL of sampled sequences as:
            </p>
            <div class="equation-container">
                <h4>$$B(x; \theta) \approx \exp\left(-\frac{1}{M} \sum_{i=1}^M \frac{1}{|y^{(i)}|}\log \tilde{P}\left(y_{1: |y^{(i)}|} | x; \theta \right)\right)$$</h4>
            </div>
            <p>
                This approach allows us to compute BF in a sample-efficient way. For task-wise BF, we simply compute it via averaging all instance-wise BF: $B(X; \theta) = \sum_{x} p(x) B(x; \theta)$.
            </p>
        </section>

        <section id="bf-measuring">
            <h2>Benchmarking and Attributing Branch Factors</h2>
            <p>
                In this part, we will introduce our BF computation experiment settings, including models, tasks, and the impact factors influencing BF.
            </p>

            <h3>Models and Sampling</h3>
            <p>
                We run experiments on models from Llama-2 <d-cite key="touvron2023llama"></d-cite> and Llama-3 <d-cite key="dubey2024llama"></d-cite> families as they are widely-used open-weight model families. For each model family, we include both base and aligned models to investigate how alignment tuning affects BF. We set $p=0.9$ and $T=1.0$ to sample outputs to conform with the setting for most datasets.
            </p>
            <p>
                We set $M=50$ sequences to estimate BF, which yields a reliable estimation across datasets in prior studies. For aligned models, we apply the official chat templates to prompts. In addition, we carefully control the lengths of all inputs plus outputs to be within the context window of the models.
            </p>

            <h3>Tasks</h3>
            <p>
                We consider a variety of tasks covering common application scenarios of LLM generation, including reasoning and open-ended generation: <strong>MMLU</strong> <d-cite key="hendrycks2021measuring"></d-cite> (Reasoning), <strong>Cognac</strong> <d-cite key="chen2022cognac"></d-cite> (Controlled Generation), <strong>BBCLatestNews</strong> <d-cite key="li2024latesteval"></d-cite> (News Generation), and <strong>Creative Story Generation</strong> <d-cite key="chakrabarty2024art"></d-cite> (Creative Generation). To test subjective randomness bias <d-cite key="bigelowsubjective"></d-cite>, we also prepare a synthetic task <strong>Random Strings</strong> where the prompt is generated via randomly sampled characters.
            </p>

            <div class="figure-group" data-group-name="BF Dynamics Across Tasks">
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization_temp/outputs/storytelling/piecewise_ebf_Llama-3-70B_perplexity_no_legend.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 3a: Creative Story Generation - Llama-3-70B Base</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization_temp/outputs/random_string/piecewise_ebf_Llama-3-70B_perplexity_no_legend.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 3b: Random Strings - Llama-3-70B Base</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization_temp/outputs/news/piecewise_ebf_Llama-3-70B_perplexity_no_legend.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 3c: BBC News - Llama-3-70B Base</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization_temp/outputs/storytelling/piecewise_ebf_Llama-3-70B-Instruct_perplexity_no_legend.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 3d: Creative Story Generation - Llama-3-70B-Instruct</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization_temp/outputs/random_string/piecewise_ebf_Llama-3-70B-Instruct_perplexity_no_legend.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 3e: Random Strings - Llama-3-70B-Instruct</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization_temp/outputs/news/piecewise_ebf_Llama-3-70B-Instruct_perplexity_no_legend.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 3f: BBC News - Llama-3-70B-Instruct</figcaption>
                    </figure>
                </d-figure>
            </div>

            <h3>Impact Factors (IFs)</h3>
            <p>
                We consider modulating these factors that may impact BF computations: <strong>Prompt Complexity</strong> ($C$), <strong>Alignment Tuning</strong> $(AT \in \{\text{Instruct},\text{Base}\})$, <strong>Model Size</strong> $(S \in \{8\text{B}/13\text{B},70\text{B}\})$, and <strong>Model Generation</strong> $(G \in \{2,3\})$. $C$ controls the informativeness of the input prompt $x$ (e.g., the number of banned words in Cognac, the number of in-context samples in MMLU). Intuitively, providing more information in $x$ should make the model more confident in its outputs, resulting in a lower BF. $AT, S, G$ represent model-wise variations to explore how different configurations of $\theta$ affect $B(X; \theta)$.
            </p>

            <h3>BF Dynamic in Generation Process</h3>
            <p>
                Both BF and the output length $N$ are functions of the output $Y$, and BF computation relies on $N$. To avoid confounding effects, we first analyze how BF varies with $N$ before intervening IFs. We demonstrate BF trajectories over different output positions by running Llama-3-70B and Llama-3-70B-Instruct on three representative tasks. Specifically, we compute BF over every five output tokens, conditioning on the prompt and all previously generated output tokens.
            </p>
            <p>
                As we can see, first, <strong>the average BF for the base model</strong> (~$\approx 12$) <strong>is roughly ten times higher than the aligned model</strong> ($\approx 1.2$). Therefore, there are actually very few candidate next-token to be truncated in decoding for the aligned models. This explains why the decoding method would assert weaker effects for aligned models. Also, in most cases, <strong>BF would often drop smoothly as more output tokens are generated</strong>. Under the same task, when $C>0$, different $C$ mainly controls the starting point and the rate of decreasing, while in the end, they would converge to roughly the same point. When almost zero knowledge is provided ($C=0$), the output will end much earlier compared to $C > 0$ cases. These findings also provide support that the future token generation is gradually becoming predictable and the model may have a certain generation plan to follow, resonating with recent observation in interpretability <d-cite key="pal2023future"></d-cite><d-cite key="wu2024language"></d-cite><d-cite key="li2024predicting"></d-cite> and inference acceleration <d-cite key="cai2024medusa"></d-cite><d-cite key="welleck2024from"></d-cite>.
            </p>

            <h3>Pareto Analysis of BF</h3>
            <p>
                We perform a Pareto analysis to identify the relative influence of all IFs of BF. For each factor $D_i$, we define the unnormalized <em>Impact</em> $\tilde{I}(D_i)$ as the average absolute pairwise difference in BF when varying $D_i$ while holding other dimensions constant:
            </p>
            <div class="equation-container">
                <h4>$$\tilde{I}(D_i) = \frac{ \sum_{d_i, d_j \in \text{Domain}(D_i), d_i \neq d_j} 
                 {|\text{Avg}(\text{B}(\cdot | D_i=d_i)) - \text{Avg}(\text{B}(\cdot | D_i=d_j))|}}{|\text{Domain}(D_i)| * |\text{Domain}(D_i) - 1|}$$</h4>
            </div>
            <p>
                Then we normalize it as ${I}(D_i)=\frac{\tilde{I}(D_i)}{\sum \tilde{I}(D_i)}$.
            </p>

            <div class="figure-group" data-group-name="Pareto Analysis Across Tasks">
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/pareto_analysis_max_length/pareto_chart_cognac.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 4a: Cognac - Pareto Analysis</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/pareto_analysis_max_length/pareto_chart_mmlu.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 4b: MMLU - Pareto Analysis</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/pareto_analysis_max_length/pareto_chart_bbcnews.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 4c: BBC News - Pareto Analysis</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/pareto_analysis_max_length/pareto_chart_storytelling.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure 4d: Creative Story Generation - Pareto Analysis</figcaption>
                    </figure>
                </d-figure>
            </div>

            <p>
                The results indicate that <strong>alignment tuning is the most influential factor affecting BF</strong>, surpassing model size, model generation, and prompt complexity by a large margin. For tasks with richer inputs--such as MMLU (with more in-context examples) and BBCLatestNews (with more headlines)--prompt complexity $C$ and model size $M$ emerge as the next most impactful factors. In contrast, for open-ended tasks like Cognac and Story Generation, model generation $G$--particularly improvements from Llama-2 to Llama-3--plays a more dominant role. This shift likely reflects gains from the use of larger, more diverse datasets in training <d-cite key="dubey2024llama"></d-cite>.
            </p>

            <h3>Curious Case of Prompt Complexity</h3>
            <p>
                Intuitively, greater prompt specificity (larger $C$) reduces BF by narrowing the model's output space through more informative context. However, our experimental results reveal task-varied effects. For the Cognac task, greater prompt complexity can <em>increase</em> BF--potentially due to the cognitive burden of processing negation or complex linguistic structures. In contrast, for tasks like News Generation, higher $C$ generally leads to lower BF, consistent with the expected narrowing of output diversity.
            </p>

        </section>

        <section id="application-variance">
            <h2>Application: Variance Reduction and Risks of Mid-Generation Forking</h2>
            <p>
                Building on our findings that BF declines over the generation process and is lower in aligned models, we derive a practical implication: aligned CoT models, by starting with low BF and delaying decisive tokens, shrink the output space more aggressively and produce fewer high-probability variants. To test this, we evaluate output variability on MMLU-STEM using 200 samples per model, measuring the standard deviation of Majority@K accuracy for $K = 1, 3, 8, 16$ under temperature $T=0.6$ and truncation threshold $p=0.9$.
            </p>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Maj@1 Std</th>
                            <th>Maj@3 Std</th>
                            <th>Maj@8 Std</th>
                            <th>Maj@16 Std</th>
                            <th>BF@1</th>
                            <th>BF</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>DeepSeek-R1-Distill-Llama-70B</td>
                            <td><strong>14.34</strong></td>
                            <td><strong>8.29</strong></td>
                            <td><strong>4.99</strong></td>
                            <td><strong>3.21</strong></td>
                            <td>1.77</td>
                            <td><strong>1.23</strong></td>
                        </tr>
                        <tr>
                            <td>Llama-3-70B-Instruct</td>
                            <td>16.37</td>
                            <td>11.40</td>
                            <td>7.50</td>
                            <td>5.12</td>
                            <td>2.44</td>
                            <td>1.28</td>
                        </tr>
                        <tr>
                            <td>Llama-3-70B</td>
                            <td>27.78</td>
                            <td>19.53</td>
                            <td>13.22</td>
                            <td>9.23</td>
                            <td>2.41</td>
                            <td>1.31</td>
                        </tr>
                        <tr>
                            <td>DeepSeek-R1-Distill-Llama-8B</td>
                            <td><strong>27.10</strong></td>
                            <td><strong>20.91</strong></td>
                            <td><strong>13.93</strong></td>
                            <td><strong>9.14</strong></td>
                            <td>1.77</td>
                            <td><strong>1.23</strong></td>
                        </tr>
                        <tr>
                            <td>Llama-3.1-8B-Instruct</td>
                            <td>31.54</td>
                            <td>24.64</td>
                            <td>17.30</td>
                            <td>12.90</td>
                            <td>2.73</td>
                            <td>1.31</td>
                        </tr>
                        <tr>
                            <td>Llama-3.1-8B</td>
                            <td>36.41</td>
                            <td>29.78</td>
                            <td>20.43</td>
                            <td>14.05</td>
                            <td>2.53</td>
                            <td>1.35</td>
                        </tr>
                    </tbody>
                </table>
                <p class="table-caption"><strong>Table 2:</strong> Majority Voting@K standard deviation on MMLU-STEM with 200 samples. We compute the standard deviation over 100 bootstrapping trials, each using 64 samples per instance. We set $T=0.6, p=0.9$ to match standard benchmarking settings. Lower temperature concentrates probability mass on fewer tokens, reducing BF and making direct comparisons more difficult. Still, BF remains a strong predictor of standard deviation.</p>
            </div>
            <p>
                As shown in Table 2, among models with similar capacity, those with lower BF--especially the aligned CoT model--exhibit markedly lower variance. This confirms that BF is a reliable predictor of sampling stability.
            </p>
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bon_from_middle_demo.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure 5: Resampling from different output positions to assess the effect of interrupting BF reduction. We resample new continuations at the 25th and 200th output token of DeepSeek-Distilled Llama-8B MMLU outputs. Results show substantial performance drops at both positions.</figcaption>
                </figure>
            </d-figure>
        </section>

        <section id="bf-nudging">
            <h2>How does Alignment Tuning Impact BF?</h2>
            <p>
                Why does alignment tuning exert such a pronounced effect on BF? Building on the superficial alignment hypothesis <d-cite key="zhou2024lima"></d-cite> ("<em>Alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users.</em>") and recent work on tuning-free alignment <d-cite key="lin2023unlocking"></d-cite><d-cite key="fei2024nudging"></d-cite>, we hypothesize that base models already encode low-entropy conditional distributions. In this view, alignment tuning doesn't reshape generation from scratch, but instead nudge the model toward stylistic tokens (e.g., "Sure"), thereby narrowing the conditional output distribution.
            </p>
            <p>
                To test this hypothesis, we reproduce the nudging experiments <d-cite key="fei2024nudging"></d-cite>, over Just-Eval-Instruct <d-cite key="lin2023unlocking"></d-cite> and MMLU datasets. We employ Llama-3-70B for drafting most outputs. However, when the base model's Top-1 probability is low, we apply nudging by switching to Llama-3-8B-Instruct to generate a single word. BF was computed as in prior experiments.
            </p>
            <p>
                The results indicate that after nudging occurs early in the generation process -- indicating the prefix generated by the nudging model is of low probability. These observations collectively support our hypothesis. Considering that nudging not only reduces BF but also improves aligned model performance on these tasks <d-cite key="fei2024nudging"></d-cite>, our results highlight the dual effect of alignment training: reducing BF while preserving or even enhancing task performance.
            </p>

            <div class="figure-group">
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/nudging_bf/nudging_just_eval_instruct_piecewise_ebf_Llama-3-70B_perplexity.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure A.4: Output Perplexity Dynamics in Nudging Experiments.</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/nudging_bf/just_eval_instruct_model_frequency_histogram.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure A.5: Nudging Ratio Histogram.</figcaption>
                    </figure>
                </d-figure>
            </div>

            <div class="figure-group" data-group-name="Appendix Nudging Experiments">
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/nudging_bf/nudging_just_eval_instruct_piecewise_ebf_Llama-3-70B_perplexity.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure A.4a: Just-Eval-Instruct - BF Dynamics with Nudging</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/nudging_bf/nudging_mmlu_wa_filler_piecewise_ebf_Llama-3-70B_perplexity.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure A.4b: MMLU - BF Dynamics with Nudging</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/nudging_bf/just_eval_instruct_model_frequency_histogram.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure A.4c: Just-Eval-Instruct - Model Frequency Histogram</figcaption>
                    </figure>
                </d-figure>
                
                <d-figure>
                    <figure>
                        <div class="img-container lazy-load" data-src="static/visualization/nudging_bf/mmlu_model_frequency_histogram.pdf">
                            <div class="loading-spinner">Loading figure...</div>
                        </div>
                        <figcaption>Figure A.4d: MMLU - Model Frequency Histogram</figcaption>
                    </figure>
                </d-figure>
            </div>
        </section>

        <!-- <section id="related-works">
            <h2>Related Works</h2>
            <h3>Uncertainty Quantification for LLM</h3>
            <p>
                Uncertainty quantification (UQ) for LLMs has gained significant attention due to its importance in real-world applications, particularly in high-stakes domains <d-cite key="desai2020calibration"></d-cite><d-cite key="jiang2021can"></d-cite><d-cite key="wang2022uncertainty"></d-cite><d-cite key="kadavath2022language"></d-cite><d-cite key="xiong2024can"></d-cite><d-cite key="ye2024benchmarking"></d-cite><d-cite key="gupta2024language"></d-cite>. Existing methods typically address closed-domain tasks such as classification and question-answering, where outputs are discrete and easier to assess. However, as <d-cite key="kuhn2023semantic"></d-cite> note, these approaches often overlook challenges specific to open-ended generation, such as semantic equivalence across outputs. They introduce "semantic entropy" to quantify uncertainty in LLM output space by first clustering the sampled output and then quantifying uncertainty over cluster distribution. This method empirically works well in hallucination detection <d-cite key="farquhar2024detecting"></d-cite>. In this paper, we focus on investigating the probability concentration phenomenon for LLMs. We introduce BF to quantify this concentration, which applies broadly across tasks without imposing strong assumptions on output categories.
            </p>

            <h3>Reduced Diversity in Aligned Models</h3>
            <p>
                Recent studies have consistently shown that alignment tuning reduces output diversity in language models <d-cite key="perez2022red"></d-cite><d-cite key="padmakumar2024does"></d-cite><d-cite key="chakrabarty2024art"></d-cite><d-cite key="tian2024large"></d-cite><d-cite key="kirk2024understanding"></d-cite><d-cite key="lu2025ai"></d-cite><d-cite key="west2025base"></d-cite>. Our work aims at connecting reduced diversity with related observations on diminished randomness and robustness in aligned models <d-cite key="saparov2023language"></d-cite><d-cite key="song2024good"></d-cite><d-cite key="renze2024effect"></d-cite><d-cite key="bigelowsubjective"></d-cite>, and proposes a unifying explanation: increased probability concentration. Traditional diversity metrics such as n-gram lexical diversity <d-cite key="li2016diversity"></d-cite> are sensitive to vocabulary size and output length <d-cite key="liu-etal-2022-rethinking"></d-cite><d-cite key="tevet2021evaluating"></d-cite><d-cite key="guo2024benchmarking"></d-cite><d-cite key="kirk2024understanding"></d-cite> and cannot work well with most recent long CoT models. In our appendix, we demonstrate that lexical diversity poorly correlates with BF and fails to robustly measure generation concentration.
            </p>

            <p>
                Our work also resonates with information density research in cognitive science and linguistic theories, and we present a short discussion in our appendix.
            </p>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>
                We investigate probability concentration in LLMs through the lens of branching factor (BF). Aligned models exhibit BF values nearly an order of magnitude lower than their base counterparts, with BF declining further during generation. This helps explain their reduced output diversity, low sampling variance, and insensitivity to decoding strategies. We predict and verify that aligned CoT models, due to their especially low BF, produce more stable outputs and mid-generation resampling yields degraded performance by forcing unlikely continuations. Nudging experiments further support our hypothesis that alignment narrows generation not by reshaping the model, but by steering it toward stylistic tokens that activate low-entropy subspaces already present in the base model.
            </p>
            <p>
                Open questions remain: which components of alignment tuning drive these effects, and how does reduced BF shape user interaction in more complex, real-world tasks? Future work may build on our findings to design inference-time decoding strategies that better balance diversity and stability.
            </p>
        </section> -->

        <section id="bf-nudging">
        <div class="links">
            <h3>Resources</h3>
            <ul>
                <li><a href="https://arxiv.org/abs/2502.16810">Paper on arXiv</a></li>
                <li><a href="">Code Repository (TBD)</a></li>
            </ul>
        </div>
        </section>
    </div>
</d-article>

<script src="js/contents_bar.js"></script>

<section id="appendix">
    <h2>Appendix</h2>
    
    <section id="appendix-case-study">
        <h3>Case Study Implementation Details</h3>
        <p>
            We use the scripts in Qwen-2.5-Math <d-cite key="yang2024qwen2"></d-cite> for standard reasoning benchmarks. We sample 200 examples from MMLU-STEM and compute the performance numbers under 64 trials and report the average performance.
        </p>
        
        <d-figure class="appendix-figure">
            <figure>
                <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-13B_perplexity.pdf">
                    <div class="loading-spinner">Loading figure...</div>
                </div>
                <figcaption>Figure A.1: BF Output Dynamic for Llama-2-families. For better visualization, we compute the exponential moving averaged values of perplexity with the smoothing factor set as 0.1.</figcaption>
            </figure>
        </d-figure>

        <div class="figure-group appendix-figure" data-group-name="Llama-2 Family BF Dynamics">
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-13B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1a: Creative Story - Llama-2-13B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/random_strings/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-13B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1b: Random Strings - Llama-2-13B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bbcnews/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-13B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1c: BBC News - Llama-2-13B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/mmlu/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-13B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1d: MMLU - Llama-2-13B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-13B-chat_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1e: Creative Story - Llama-2-13B-Chat</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/random_strings/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-13B-chat_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1f: Random Strings - Llama-2-13B-Chat</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bbcnews/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-13B-chat_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1g: BBC News - Llama-2-13B-Chat</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/mmlu/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-13B-chat_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1h: MMLU - Llama-2-13B-Chat</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-70B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1i: Creative Story - Llama-2-70B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/random_strings/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-70B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1j: Random Strings - Llama-2-70B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bbcnews/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-70B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1k: BBC News - Llama-2-70B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/mmlu/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-70B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1l: MMLU - Llama-2-70B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-70B-chat_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1m: Creative Story - Llama-2-70B-Chat</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/random_strings/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-70B-chat_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1n: Random Strings - Llama-2-70B-Chat</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bbcnews/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-70B-chat_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1o: BBC News - Llama-2-70B-Chat</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/mmlu/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-2-70B-chat_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.1p: MMLU - Llama-2-70B-Chat</figcaption>
                </figure>
            </d-figure>
        </div>
    </section>

    <section id="appendix-dataset-details">
        <h3>Dataset-Specific Processing</h3>
        <p>
            For all datasets we used in the paper, we carefully controlled whether the prompt length and expected output length would exceed the model's maximum length.
        </p>
        
        <h4>MMLU</h4>
        <p>
            <d-cite key="hendrycks2021measuring"></d-cite> is a widely-used multiple-choice reasoning question. Unless otherwise explained, we use the full test set of MMLU to avoid potential contamination, following benchmarking settings reported in most LLM technical reports <d-cite key="touvron2023llama"></d-cite><d-cite key="dubey2024llama"></d-cite><d-cite key="guo2025deepseek"></d-cite>. We formulate prompt complexity $C$ as the number of in-context samples. For example, $C=1$ means we only add one in-context sample. For prompting setup and postprocessing details, we follow the standard implementation in Qwen-2.5-Math <d-cite key="yang2024qwen2"></d-cite>.
        </p>
        
        <h4>Cognac</h4>
        <p>
            <d-cite key="chen2022cognac"></d-cite> is a controlled generation task requiring language model <em>not</em> to generate specified banned words provided in the prompt. We use the WordNet subset <d-cite key="miller1995wordnet"></d-cite> of Cognac as this is the only released setting in Cognac paper, where the topic is a root node and the constraint is defined as a subtree. We sampled 200 instances using the provided data generation codes in our experiments. To ensure most model generations ended properly in the decoding process, we relax the constraint of maximum decoded tokens $T$ from 60 to 512. We use the same prompt templates following their Github repo.
        </p>
        
        <h4>Creative Story Generation</h4>
        <p>
            <d-cite key="chakrabarty2024art"></d-cite> provides the plots and story continuation from both machine and human. We adopt the provided 11 human-written story plots in the original dataset as the prompt. In this task, we set the maximum token $T=1024$ to ensure the continued story written by LLM can have a proper ending. We formulate prompt complexity $C$ as providing $C \times 25$ words in the plot.
        </p>
        
        <h4>Random Strings</h4>
        <p>
            Similar to <d-cite key="bigelowsubjective"></d-cite>, we sample 200 random strings with length $L\sim U(256, 512)$ from the tokenizer vocabulary as the prompt. Prompt complexity $C$ is formulated by providing $C \times 15$ tokens in the prompt, ensuring each article contains at least 100 tokens.
        </p>
        
        <h4>BBCLatestNews</h4>
        <p>
            <d-cite key="li2024latesteval"></d-cite> is a news collection dataset aims at collecting news that is beyond the time cut for training LLMs. Unlike creative story plots, news articles are typically more structured and organized, although headlines can still be surprising. We select news articles from January to July 2024 to minimize data contamination, as the Llama models have a knowledge cut-off in late 2023. We formulate prompt complexity $C$ as providing $C \times 15$ words in the plot.
        </p>
        
        <d-figure class="appendix-figure">
            <figure>
                <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-8B_perplexity.pdf">
                    <div class="loading-spinner">Loading figure...</div>
                </div>
                <figcaption>Figure A.2: BF Output Dynamic for Llama-3-families. For better visualization, we compute the exponential moving averaged values of perplexity with the smoothing factor set as 0.1.</figcaption>
            </figure>
        </d-figure>

        <div class="figure-group appendix-figure" data-group-name="Llama-3 Family BF Dynamics">
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-8B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2a: Creative Story - Llama-3-8B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/random_strings/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-8B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2b: Random Strings - Llama-3-8B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bbcnews/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-8B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2c: BBC News - Llama-3-8B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/mmlu/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-8B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2d: MMLU - Llama-3-8B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-70B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2e: Creative Story - Llama-3-70B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/random_strings/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-70B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2f: Random Strings - Llama-3-70B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bbcnews/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-70B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2g: BBC News - Llama-3-70B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/mmlu/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-70B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2h: MMLU - Llama-3-70B Base</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-8B-Instruct_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2i: Creative Story - Llama-3-8B-Instruct</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/random_strings/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-8B-Instruct_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2j: Random Strings - Llama-3-8B-Instruct</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bbcnews/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-8B-Instruct_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2k: BBC News - Llama-3-8B-Instruct</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/mmlu/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-8B-Instruct_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2l: MMLU - Llama-3-8B-Instruct</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-70B-Instruct_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2m: Creative Story - Llama-3-70B-Instruct</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/random_strings/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-70B-Instruct_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2n: Random Strings - Llama-3-70B-Instruct</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bbcnews/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-70B-Instruct_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2o: BBC News - Llama-3-70B-Instruct</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/mmlu/visualization_promptwise_0_smoothing_0.1/piecewise_ebf_Llama-3-70B-Instruct_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.2p: MMLU - Llama-3-70B-Instruct</figcaption>
                </figure>
            </d-figure>
        </div>
    </section>

    <section id="appendix-full-output-bf">
        <h3>Full BF Output Dynamics Investigation</h3>
        <p>
            For better visualization, we would submit all full-size figures for task-wise and model-wise BF output dynamics in zipped supplementary materials. Here we only present full task-wise (except Cognac, which would be hard to read if we put it here) and model-wise BF output dynamic for Llama-2 in Figure A.1 and Llama-3 in Figure A.2. We can observe the trends as in the main text:
        </p>
        <ol>
            <li><strong>The average BF for the base model</strong> (~$\approx 12$) <strong>is roughly ten times higher than the aligned model</strong> ($\approx 1.2$).</li>
            <li><strong>BF would often drop smoothly as more output tokens are generated</strong>.</li>
        </ol>
    </section>

    <section id="appendix-full-taskwise-bf">
        <h3>Full Task-wise BF Evaluation on Different Prompt Complexity</h3>
        <p>
            The full task-wise BF evaluation results over different prompt complexity can be found in Figure A.3. Here we can see that prompt complexity modulates BF in highly non-consistent ways across models and tasks, and there are no clear monotonic patterns, contradicting the intuition that with more context given, the model should have more confidence in what to generate.
        </p>
        
        <d-figure class="appendix-figure">
            <figure>
                <div class="img-container lazy-load" data-src="static/visualization/cognac/visualization_promptwise_0_smoothing_1/model_wise_comparison_ebf_perplexity.pdf">
                    <div class="loading-spinner">Loading figure...</div>
                </div>
                <figcaption>Figure A.3: BF changes with prompt complexity ($C$) for Different Tasks. We can see prompt complexity affects BF in a task-varied way.</figcaption>
            </figure>
        </d-figure>

        <div class="figure-group appendix-figure" data-group-name="Task-wise BF Evaluation">
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/cognac/visualization_promptwise_0_smoothing_1/model_wise_comparison_ebf_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.3a: Cognac - BF vs Prompt Complexity</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/storytelling/visualization_promptwise_0_smoothing_1/model_wise_comparison_plot_ebf_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.3b: Creative Story Generation - BF vs Prompt Complexity</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/random_strings/visualization_promptwise_0_smoothing_1/model_wise_comparison_ebf_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.3c: Random Strings - BF vs Prompt Complexity</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/bbcnews/visualization_promptwise_0_smoothing_1/model_wise_comparison_ebf_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.3d: BBC News - BF vs Prompt Complexity</figcaption>
                </figure>
            </d-figure>
        </div>
    </section>

    <section id="appendix-aep-proof">
        <h3>Proof of AEP and other corollaries</h3>
        <h4>Proof of AEP</h4>
        <p>
            The proof below is a simplified version of the proof in <d-cite key="mudireddy2024slaves"></d-cite>. For more formal measure-theoretical proof, we refer readers to their original papers for more details. We present the simplified proof here mostly for completeness of the paper and to fix some minor issues when deriving bounds.
        </p>
        <p>
            The key observation here is that under current computation architecture, the probability implemented by transformers are log-precision <d-cite key="merrill2023parallelism"></d-cite>, and thus $|\log {P}\left(y_{1: N} | x; \theta \right)|$ is bounded (e.g., $|\log {P}\left(y_{1: N} | x; \theta \right)| \leq M$. For the truncated probability $\tilde{P}\left(y_{1: N} | x; \theta \right)$, we can essentially only consider the non-zero probability over truncated vocabulary and the same thing holds. Depending on the quantization scheme implemented, examples of $M$ include $32, 64$, etc.
        </p>
        <p>
            Then, note that ${\bar{H}\left(Y_{1: N} | x; \theta \right)}$ is actually the first moment of variable $\log {P}\left(y_{1: N} | x; \theta \right)$ divided by $N$, if we can further show that the second moment of $\log {P}\left(y_{1: N} | x; \theta \right)$ is bounded by some $G(N)$ which only increases linearly in $N$, by Chebyshev Inequality <d-cite key="cohen2015markov"></d-cite>, we can prove:
        </p>
        <div class="equation-container">
            <h4>$$P\left( \left\lvert -\frac{1}{N}\log \tilde{P}\left(y_{1: N} | x; \theta \right) - {\bar{H}\left(Y_{1: N} | x; \theta \right)}\right\rvert \geq \epsilon \right) \leq \frac{\text{Var}(\log {P}\left(y_{1: N} | x; \theta \right))}{N^2\epsilon^2} \leq \frac{G(N)}{N^2\epsilon^2}$$</h4>
        </div>
        <p>
            Then taking $N\rightarrow \infty$ on both sides, we will see:
        </p>
        <div class="equation-container">
            <h4>$$P\left( \left\lvert -\frac{1}{N}\log \tilde{P}\left(y_{1: N} | x; \theta \right) - {\bar{H}\left(Y_{1: N} | x; \theta \right)}\right\rvert \geq \epsilon \right) \rightarrow 0$$</h4>
        </div>
        <p>
            Equivalently:
        </p>
        <div class="equation-container">
            <h4>$$\lim_{N \rightarrow \infty}{P\left( \left\lvert -\frac{1}{N}\log \tilde{P}\left(y_{1: N} | x; \theta \right) - {\bar{H}\left(Y_{1: N} | x; \theta \right)}\right\rvert < \epsilon \right) } = 1$$</h4>
        </div>
        <p>
            and we can complete the proof.
        </p>
        <p>
            Now let's prove a specific choice of $G(N)=NM^2$ upper-bounds $\text{Var}\left[\log{P}\left(y_{1: N} | x; \theta \right)\right]$.
        </p>
        <p>
            First, we note that:
        </p>
        <div class="equation-container">
            <h4>$$\log{P}\left(y_{1: N} | x; \theta \right) = \sum_{t=1}^N \log{P}\left(y_{t} | [x; y_{1:t-1}]; \theta \right)$$</h4>
        </div>
        <div class="equation-container">
            <h4>$${H}\left(Y_{1: N} | x; \theta \right) = \sum_{t=1}^N {H}\left(Y_{t} | [x; Y_{1:t-1}]; \theta \right) = \sum_{t=1}^N \mathbb{E}_{y_{1:t-1}} \left[H\left(Y_{t} | [x, y_{1:t-1}]; \theta \right)\right]$$</h4>
        </div>
        <p>
            Following <d-cite key="mudireddy2024slaves"></d-cite>, we use induction on $N$ to prove $\text{Var}\left[\log{P}\left(y_{1: N} | x; \theta \right)\right]\leq NM^2$. The complete proof involves showing that the cross-product terms in the variance expansion are zero due to the law of iterated expectation, which completes the induction.
        </p>
    </section>

    <section id="appendix-nudging">
        <h3>Full Nudging Experiment Results</h3>
        <p>
            Due to space limits, we put the nudging experiment results for MMLU here. Though on MMLU, nudging does not reduce BF that quickly as over Just-Eval-Instruct, it does bring down BF of base models significantly, which verifies our hypothesis in the main text.
        </p>
        
        <div class="figure-group">
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/nudging_bf/nudging_just_eval_instruct_piecewise_ebf_Llama-3-70B_perplexity.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.4: Output Perplexity Dynamics in Nudging Experiments.</figcaption>
                </figure>
            </d-figure>
            
            <d-figure>
                <figure>
                    <div class="img-container lazy-load" data-src="static/visualization/nudging_bf/just_eval_instruct_model_frequency_histogram.pdf">
                        <div class="loading-spinner">Loading figure...</div>
                    </div>
                    <figcaption>Figure A.5: Nudging Ratio Histogram.</figcaption>
                </figure>
            </d-figure>
        </div>
    </section>

    <section id="appendix-bf-and-id">
        <h3>BF and Information Density</h3>
        <p>
            Our BF measure can also be interpreted as capturing the information density that LLMs target to facilitate efficient communication <d-cite key="genzel2002entropy"></d-cite><d-cite key="jaeger2006speakers"></d-cite><d-cite key="levy2008expectation"></d-cite><d-cite key="mahowald2013info"></d-cite><d-cite key="meister2021revisiting"></d-cite><d-cite key="verma2023revisiting"></d-cite>. Prior work has leveraged both token-level log-probabilities and entropy rates ($\bar{H}$) as proxies for information density in human and machine communication. In our main theorem, we formalize the connection between these views, showing that BF--defined as the exponentiated entropy rate--aligns naturally with this theoretical framework. Unlike prior studies focused primarily on linguistic theory or cognitive science, our work operationalizes this principle at scale across modern LLMs, linking information density to alignment training, decoding dynamics, and output variability in a unified analysis.
        </p>
    </section>
</section>

</body>
</html>